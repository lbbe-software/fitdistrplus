<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Which optimization algorithm to choose? • fitdistrplus</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Which optimization algorithm to choose?">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">fitdistrplus</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.2-1</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/fitdistrplus_vignette.html">Overview of the fitdistrplus package</a></li>
    <li><a class="dropdown-item" href="../articles/FAQ.html">Frequently Asked Questions</a></li>
    <li><a class="dropdown-item" href="../articles/Optimalgo.html">Which optimization algorithm to choose?</a></li>
    <li><a class="dropdown-item" href="../articles/starting-values.html">Starting values used in fitdistrplus</a></li>
  </ul>
</li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/lbbe-software/fitdistrplus/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">


<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Which optimization algorithm to choose?</h1>
                        <h4 data-toc-skip class="author">Marie Laure Delignette Muller, Christophe Dutang</h4>
            
            <h4 data-toc-skip class="date">2024-07-15</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/lbbe-software/fitdistrplus/blob/master/vignettes/Optimalgo.Rmd" class="external-link"><code>vignettes/Optimalgo.Rmd</code></a></small>
      <div class="d-none name"><code>Optimalgo.Rmd</code></div>
    </div>

    
    
<div class="section level2" number="1">
<h2 id="quick-overview-of-main-optimization-methods">
<span class="header-section-number">1</span> Quick overview of main optimization methods<a class="anchor" aria-label="anchor" href="#quick-overview-of-main-optimization-methods"></a>
</h2>
<p>We present very quickly the main optimization methods.
Please refer to <strong>Numerical Optimization (Nocedal &amp; Wright, 2006)</strong>
or <strong>Numerical Optimization: theoretical and practical aspects
(Bonnans, Gilbert, Lemarechal &amp; Sagastizabal, 2006)</strong> for a good introduction.
We consider the following problem <span class="math inline">\(\min_x f(x)\)</span> for <span class="math inline">\(x\in\mathbb{R}^n\)</span>.</p>
<div class="section level3" number="1.1">
<h3 id="derivative-free-optimization-methods">
<span class="header-section-number">1.1</span> Derivative-free optimization methods<a class="anchor" aria-label="anchor" href="#derivative-free-optimization-methods"></a>
</h3>
<p>The Nelder-Mead method is one of the most well known derivative-free methods
that use only values of <span class="math inline">\(f\)</span> to search for the minimum.
It consists in building a simplex of <span class="math inline">\(n+1\)</span> points and moving/shrinking
this simplex into the good direction.</p>
<ol style="list-style-type: decimal">
<li>set initial points <span class="math inline">\(x_1, \dots, x_{n+1}\)</span>.</li>
<li>order points such that <span class="math inline">\(f(x_1)\leq f(x_2)\leq\dots\leq f(x_{n+1})\)</span>.</li>
<li>compute <span class="math inline">\(x_o\)</span> as the centroid of <span class="math inline">\(x_1, \dots, x_{n}\)</span>.</li>
<li>Reflection:
<ul>
<li>compute the reflected point <span class="math inline">\(x_r = x_o + \alpha(x_o-x_{n+1})\)</span>.</li>
<li>
<strong>if</strong> <span class="math inline">\(f(x_1)\leq f(x_r)&lt;f(x_n)\)</span>,
then replace <span class="math inline">\(x_{n+1}\)</span> by <span class="math inline">\(x_r\)</span>,
go to step 2.</li>
<li>
<strong>else</strong> go step 5.</li>
</ul>
</li>
<li>Expansion:
<ul>
<li>
<strong>if</strong> <span class="math inline">\(f(x_r)&lt;f(x_1)\)</span>, then compute the expansion point
<span class="math inline">\(x_e= x_o+\gamma(x_o-x_{n+1})\)</span>.</li>
<li>
<strong>if</strong> <span class="math inline">\(f(x_e) &lt;f(x_r)\)</span>, then replace <span class="math inline">\(x_{n+1}\)</span> by <span class="math inline">\(x_e\)</span>,
go to step 2.</li>
<li>
<strong>else</strong> <span class="math inline">\(x_{n+1}\)</span> by <span class="math inline">\(x_r\)</span>, go to step 2.</li>
<li>
<strong>else</strong> go to step 6.</li>
</ul>
</li>
<li>Contraction:
<ul>
<li>compute the contracted point <span class="math inline">\(x_c = x_o + \beta(x_o-x_{n+1})\)</span>.</li>
<li>
<strong>if</strong> <span class="math inline">\(f(x_c)&lt;f(x_{n+1})\)</span>,
then replace <span class="math inline">\(x_{n+1}\)</span> by <span class="math inline">\(x_c\)</span>,
go to step 2.<br>
</li>
<li>
<strong>else</strong> go step 7.</li>
</ul>
</li>
<li>Reduction:
<ul>
<li>for <span class="math inline">\(i=2,\dots, n+1\)</span>, compute <span class="math inline">\(x_i = x_1+\sigma(x_i-x_{1})\)</span>.</li>
</ul>
</li>
</ol>
<p>The Nelder-Mead method is available in <code>optim</code>.
By default, in <code>optim</code>, <span class="math inline">\(\alpha=1\)</span>, <span class="math inline">\(\beta=1/2\)</span>, <span class="math inline">\(\gamma=2\)</span> and <span class="math inline">\(\sigma=1/2\)</span>.</p>
</div>
<div class="section level3" number="1.2">
<h3 id="hessian-free-optimization-methods">
<span class="header-section-number">1.2</span> Hessian-free optimization methods<a class="anchor" aria-label="anchor" href="#hessian-free-optimization-methods"></a>
</h3>
<p>For smooth non-linear function, the following method is generally used:
a local method combined with line search work on the scheme <span class="math inline">\(x_{k+1} =x_k + t_k d_{k}\)</span>, where the local method will specify the direction <span class="math inline">\(d_k\)</span> and the line search will specify the step size <span class="math inline">\(t_k \in \mathbb{R}\)</span>.</p>
<div class="section level4" number="1.2.1">
<h4 id="computing-the-direction-d_k">
<span class="header-section-number">1.2.1</span> Computing the direction <span class="math inline">\(d_k\)</span><a class="anchor" aria-label="anchor" href="#computing-the-direction-d_k"></a>
</h4>
<p>A desirable property for <span class="math inline">\(d_k\)</span> is that <span class="math inline">\(d_k\)</span> ensures a descent <span class="math inline">\(f(x_{k+1}) &lt; f(x_{k})\)</span>.
Newton methods are such that <span class="math inline">\(d_k\)</span> minimizes a local quadratic approximation of <span class="math inline">\(f\)</span> based on a Taylor expansion, that is <span class="math inline">\(q_f(d) = f(x_k) + g(x_k)^Td +\frac{1}{2} d^T H(x_k) d\)</span> where <span class="math inline">\(g\)</span> denotes the gradient and <span class="math inline">\(H\)</span> denotes the Hessian.</p>
<p>The consists in using the exact solution of local minimization problem <span class="math inline">\(d_k = - H(x_k)^{-1} g(x_k)\)</span>.<br>
In practice, other methods are preferred (at least to ensure positive definiteness).
The method approximates the Hessian by a matrix <span class="math inline">\(H_k\)</span> as a function of <span class="math inline">\(H_{k-1}\)</span>, <span class="math inline">\(x_k\)</span>, <span class="math inline">\(f(x_k)\)</span> and then <span class="math inline">\(d_k\)</span> solves the system <span class="math inline">\(H_k d = -  g(x_k)\)</span>.
Some implementation may also directly approximate the inverse of the Hessian <span class="math inline">\(W_k\)</span> in order to compute <span class="math inline">\(d_k = -W_k g(x_k)\)</span>. Using the Sherman-Morrison-Woodbury formula, we can switch between <span class="math inline">\(W_k\)</span> and <span class="math inline">\(H_k\)</span>.</p>
<p>To determine <span class="math inline">\(W_k\)</span>, first it must verify the secant equation <span class="math inline">\(H_k y_k =s_k\)</span> or <span class="math inline">\(y_k=W_k s_k\)</span> where <span class="math inline">\(y_k = g_{k+1}-g_k\)</span> and <span class="math inline">\(s_k=x_{k+1}-x_k\)</span>. To define the <span class="math inline">\(n(n-1)\)</span> terms, we generally impose a symmetry and a minimum distance conditions. We say we have a rank 2 update if <span class="math inline">\(H_k = H_{k-1} + a u u^T + b v v^T\)</span> and a rank 1 update if $H_k = H_{k-1} + a u u^T $. Rank <span class="math inline">\(n\)</span> update is justified by the spectral decomposition theorem.</p>
<p>There are two rank-2 updates which are symmetric and preserve positive definiteness</p>
<ul>
<li>DFP minimizes <span class="math inline">\(\min || H - H_k ||_F\)</span> such that <span class="math inline">\(H=H^T\)</span>:
<span class="math display">\[
H_{k+1} = \left (I-\frac {y_k s_k^T} {y_k^T s_k} \right ) H_k \left (I-\frac {s_k y_k^T} {y_k^T s_k} \right )+\frac{y_k y_k^T} {y_k^T s_k}  
\Leftrightarrow
W_{k+1} = W_k +  \frac{s_k s_k^T}{y_k^{T} s_k} - \frac {W_k y_k y_k^T W_k^T} {y_k^T W_k y_k} .
\]</span><br>
</li>
<li>BFGS minimizes <span class="math inline">\(\min || W - W_k ||_F\)</span> such that <span class="math inline">\(W=W^T\)</span>:
<span class="math display">\[
H_{k+1} = H_k - \frac{ H_k y_k y_k^T H_k }{ y_k^T H_k y_k }  + \frac{ s_k s_k^T }{ y_k^T s_k }
\Leftrightarrow
W_{k+1} = \left (I-\frac {y_k s_k^T} {y_k^T s_k} \right )^T W_k \left (I-\frac { y_k s_k^T} {y_k^T s_k} \right )+\frac{s_k s_k^T} {y_k^T s_k} .
\]</span>
</li>
</ul>
<p>In <code>R</code>, the so-called BFGS scheme is implemented in <code>optim</code>.</p>
<p>Another possible method (which is initially arised from quadratic problems)
is the nonlinear conjugate gradients.
This consists in computing directions <span class="math inline">\((d_0, \dots, d_k)\)</span> that are conjugate
with respect to a matrix close to the true Hessian <span class="math inline">\(H(x_k)\)</span>.
Directions are computed iteratively by <span class="math inline">\(d_k = -g(x_k) + \beta_k d_{k-1}\)</span> for <span class="math inline">\(k&gt;1\)</span>, once initiated by <span class="math inline">\(d_1 = -g(x_1)\)</span>.
<span class="math inline">\(\beta_k\)</span> are updated according a scheme:</p>
<ul>
<li>
<span class="math inline">\(\beta_k = \frac{ g_k^T g_k}{g_{k-1}^T g_{k-1} }\)</span>: Fletcher-Reeves update,</li>
<li>
<span class="math inline">\(\beta_k = \frac{ g_k^T (g_k-g_{k-1} )}{g_{k-1}^T g_{k-1}}\)</span>: Polak-Ribiere update.</li>
</ul>
<p>There exists also three-term formula for computing direction
<span class="math inline">\(d_k = -g(x_k) + \beta_k d_{k-1}+\gamma_{k} d_t\)</span> for <span class="math inline">\(t&lt;k\)</span>.
A possible scheme is the Beale-Sorenson update defined as
<span class="math inline">\(\beta_k = \frac{ g_k^T (g_k-g_{k-1} )}{d^T_{k-1}(g_{k}- g_{k-1})}\)</span>
and <span class="math inline">\(\gamma_k = \frac{ g_k^T (g_{t+1}-g_{t} )}{d^T_{t}(g_{t+1}- g_{t})}\)</span> if <span class="math inline">\(k&gt;t+1\)</span> otherwise <span class="math inline">\(\gamma_k=0\)</span> if <span class="math inline">\(k=t\)</span>.
See Yuan (2006) for other well-known schemes such as Hestenses-Stiefel, Dixon or Conjugate-Descent.
The three updates (Fletcher-Reeves, Polak-Ribiere, Beale-Sorenson) of the (non-linear) conjugate gradient are available in <code>optim</code>.</p>
</div>
<div class="section level4" number="1.2.2">
<h4 id="computing-the-stepsize-t_k">
<span class="header-section-number">1.2.2</span> Computing the stepsize <span class="math inline">\(t_k\)</span><a class="anchor" aria-label="anchor" href="#computing-the-stepsize-t_k"></a>
</h4>
<p>Let <span class="math inline">\(\phi_k(t) = f(x_k + t d_k)\)</span> for a given direction/iterate <span class="math inline">\((d_k, x_k)\)</span>.
We need to find conditions to find a satisfactory stepsize <span class="math inline">\(t_k\)</span>. In literature, we consider the descent condition: <span class="math inline">\(\phi_k'(0) &lt; 0\)</span>
and the Armijo condition: <span class="math inline">\(\phi_k(t) \leq \phi_k(0) + t c_1 \phi_k'(0)\)</span> ensures a decrease of <span class="math inline">\(f\)</span>.
Nocedal &amp; Wright (2006) presents a backtracking (or geometric) approach satisfying the Armijo condition and minimal condition, i.e. Goldstein and Price condition.</p>
<ul>
<li>set <span class="math inline">\(t_{k,0}\)</span> e.g. 1, <span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>,</li>
<li>
<strong>Repeat</strong> until Armijo satisfied,
<ul>
<li>
<span class="math inline">\(t_{k,i+1} =  \alpha \times t_{k,i}\)</span>.</li>
</ul>
</li>
<li><strong>end Repeat</strong></li>
</ul>
<p>This backtracking linesearch is available in <code>optim</code>.</p>
</div>
</div>
<div class="section level3" number="1.3">
<h3 id="benchmark">
<span class="header-section-number">1.3</span> Benchmark<a class="anchor" aria-label="anchor" href="#benchmark"></a>
</h3>
<p>To simplify the benchmark of optimization methods, we create a <code>fitbench</code> function that computes
the desired estimation method for all optimization methods.
This function is currently not exported in the package.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a>fitbench <span class="ot">&lt;-</span> <span class="cf">function</span>(data, distr, method, <span class="at">grad =</span> <span class="cn">NULL</span>, </span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a>                     <span class="at">control =</span> <span class="fu">list</span>(<span class="at">trace =</span> <span class="dv">0</span>, <span class="at">REPORT =</span> <span class="dv">1</span>, <span class="at">maxit =</span> <span class="dv">1000</span>), </span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a>                     <span class="at">lower =</span> <span class="sc">-</span><span class="cn">Inf</span>, <span class="at">upper =</span> <span class="sc">+</span><span class="cn">Inf</span>, ...) </span></code></pre></div>
</div>
</div>
<div class="section level2" number="2">
<h2 id="numerical-illustration-with-the-beta-distribution">
<span class="header-section-number">2</span> Numerical illustration with the beta distribution<a class="anchor" aria-label="anchor" href="#numerical-illustration-with-the-beta-distribution"></a>
</h2>
<div class="section level3" number="2.1">
<h3 id="log-likelihood-function-and-its-gradient-for-beta-distribution">
<span class="header-section-number">2.1</span> Log-likelihood function and its gradient for beta distribution<a class="anchor" aria-label="anchor" href="#log-likelihood-function-and-its-gradient-for-beta-distribution"></a>
</h3>
<div class="section level4" number="2.1.1">
<h4 id="theoretical-value">
<span class="header-section-number">2.1.1</span> Theoretical value<a class="anchor" aria-label="anchor" href="#theoretical-value"></a>
</h4>
<p>The density of the beta distribution is given by
<span class="math display">\[
f(x; \delta_1,\delta_2) = \frac{x^{\delta_1-1}(1-x)^{\delta_2-1}}{\beta(\delta_1,\delta_2)},
\]</span>
where <span class="math inline">\(\beta\)</span> denotes the beta function, see the NIST Handbook of mathematical functions <a href="https://dlmf.nist.gov/" class="external-link uri">https://dlmf.nist.gov/</a>.
We recall that <span class="math inline">\(\beta(a,b)=\Gamma(a)\Gamma(b)/\Gamma(a+b)\)</span>.
There the log-likelihood for a set of observations <span class="math inline">\((x_1,\dots,x_n)\)</span> is
<span class="math display">\[
\log L(\delta_1,\delta_2) = (\delta_1-1)\sum_{i=1}^n\log(x_i)+ (\delta_2-1)\sum_{i=1}^n\log(1-x_i)+ n \log(\beta(\delta_1,\delta_2))
\]</span>
The gradient with respect to <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is
<span class="math display">\[
\nabla \log L(\delta_1,\delta_2) =
\left(\begin{matrix}
\sum\limits_{i=1}^n\ln(x_i) - n\psi(\delta_1)+n\psi( \delta_1+\delta_2)  \\
\sum\limits_{i=1}^n\ln(1-x_i)- n\psi(\delta_2)+n\psi( \delta_1+\delta_2)
\end{matrix}\right),
\]</span>
where <span class="math inline">\(\psi(x)=\Gamma'(x)/\Gamma(x)\)</span> is the digamma function,
see the NIST Handbook of mathematical functions <a href="https://dlmf.nist.gov/" class="external-link uri">https://dlmf.nist.gov/</a>.</p>
</div>
<div class="section level4" number="2.1.2">
<h4 id="r-implementation">
<span class="header-section-number">2.1.2</span> <code>R</code> implementation<a class="anchor" aria-label="anchor" href="#r-implementation"></a>
</h4>
<p>As in the <code>fitdistrplus</code> package, we minimize the opposite of the log-likelihood:
we implement the opposite of the gradient in <code>grlnL</code>. Both the log-likelihood and its gradient
are not exported.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lnL</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">fix.arg</span>, <span class="va">obs</span>, <span class="va">ddistnam</span><span class="op">)</span> </span>
<span>  <span class="fu">fitdistrplus</span><span class="fu">:::</span><span class="fu">loglikelihood</span><span class="op">(</span><span class="va">par</span>, <span class="va">fix.arg</span>, <span class="va">obs</span>, <span class="va">ddistnam</span><span class="op">)</span> </span>
<span><span class="va">grlnlbeta</span> <span class="op">&lt;-</span> <span class="fu">fitdistrplus</span><span class="fu">:::</span><span class="va">grlnlbeta</span></span></code></pre></div>
</div>
</div>
<div class="section level3" number="2.2">
<h3 id="random-generation-of-a-sample">
<span class="header-section-number">2.2</span> Random generation of a sample<a class="anchor" aria-label="anchor" href="#random-generation-of-a-sample"></a>
</h3>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#(1) beta distribution</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html" class="external-link">rbeta</a></span><span class="op">(</span><span class="va">n</span>, <span class="fl">3</span>, <span class="fl">3</span><span class="op">/</span><span class="fl">4</span><span class="op">)</span></span>
<span><span class="fu">grlnlbeta</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">4</span><span class="op">)</span>, <span class="va">x</span><span class="op">)</span> <span class="co">#test</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] -133  317</span></span></code></pre>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html" class="external-link">hist</a></span><span class="op">(</span><span class="va">x</span>, prob<span class="op">=</span><span class="cn">TRUE</span>, xlim<span class="op">=</span><span class="fl">0</span><span class="op">:</span><span class="fl">1</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html" class="external-link">density</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/curve.html" class="external-link">curve</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Beta.html" class="external-link">dbeta</a></span><span class="op">(</span><span class="va">x</span>, <span class="fl">3</span>, <span class="fl">3</span><span class="op">/</span><span class="fl">4</span><span class="op">)</span>, col<span class="op">=</span><span class="st">"green"</span>, add<span class="op">=</span><span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span><span class="st">"topleft"</span>, lty<span class="op">=</span><span class="fl">1</span>, col<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"red"</span>,<span class="st">"green"</span><span class="op">)</span>, legend<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"empirical"</span>, <span class="st">"theoretical"</span><span class="op">)</span>, bty<span class="op">=</span><span class="st">"n"</span><span class="op">)</span></span></code></pre></div>
<p><img src="Optimalgo_files/figure-html/unnamed-chunk-4-1.png"><!-- --></p>
</div>
<div class="section level3" number="2.3">
<h3 id="fit-beta-distribution">
<span class="header-section-number">2.3</span> Fit Beta distribution<a class="anchor" aria-label="anchor" href="#fit-beta-distribution"></a>
</h3>
<p>Define control parameters.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ctr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>trace<span class="op">=</span><span class="fl">0</span>, REPORT<span class="op">=</span><span class="fl">1</span>, maxit<span class="op">=</span><span class="fl">1000</span><span class="op">)</span></span></code></pre></div>
<p>Call <code>mledist</code> with the default optimization function
(<code>optim</code> implemented in <code>stats</code> package)
with and without the gradient for the different optimization methods.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">unconstropt</span> <span class="op">&lt;-</span> <span class="fu">fitbench</span><span class="op">(</span><span class="va">x</span>, <span class="st">"beta"</span>, <span class="st">"mle"</span>, grad<span class="op">=</span><span class="va">grlnlbeta</span>, lower<span class="op">=</span><span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##     BFGS       NM     CGFR     CGPR     CGBS L-BFGS-B     NM-B   G-BFGS </span></span>
<span><span class="co">##       14       14       14       14       14       14       14       14 </span></span>
<span><span class="co">##   G-CGFR   G-CGPR   G-CGBS G-BFGS-B   G-NM-B G-CGFR-B G-CGPR-B G-CGBS-B </span></span>
<span><span class="co">##       14       14       14       14       14       14       14       14</span></span></code></pre>
<p>In the case of constrained optimization, <code>mledist</code> permits the direct use
of <code>constrOptim</code> function (still implemented in <code>stats</code> package)
that allow linear inequality constraints by using a logarithmic barrier.</p>
<p>Use a exp/log transformation of the shape parameters <span class="math inline">\(\delta_1\)</span> and <span class="math inline">\(\delta_2\)</span>
to ensure that the shape parameters are strictly positive.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dbeta2</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">shape1</span>, <span class="va">shape2</span>, <span class="va">log</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/Beta.html" class="external-link">dbeta</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">shape1</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">shape2</span><span class="op">)</span>, log<span class="op">=</span><span class="va">log</span><span class="op">)</span></span>
<span><span class="co">#take the log of the starting values</span></span>
<span><span class="va">startarg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span><span class="fu">fitdistrplus</span><span class="fu">:::</span><span class="fu">startargdefault</span><span class="op">(</span><span class="va">x</span>, <span class="st">"beta"</span><span class="op">)</span>, <span class="va">log</span><span class="op">)</span></span>
<span><span class="co">#redefine the gradient for the new parametrization</span></span>
<span><span class="va">grbetaexp</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">obs</span>, <span class="va">...</span><span class="op">)</span> </span>
<span>    <span class="fu">grlnlbeta</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">par</span><span class="op">)</span>, <span class="va">obs</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">par</span><span class="op">)</span></span>
<span>    </span>
<span></span>
<span><span class="va">expopt</span> <span class="op">&lt;-</span> <span class="fu">fitbench</span><span class="op">(</span><span class="va">x</span>, distr<span class="op">=</span><span class="st">"beta2"</span>, method<span class="op">=</span><span class="st">"mle"</span>, grad<span class="op">=</span><span class="va">grbetaexp</span>, start<span class="op">=</span><span class="va">startarg</span><span class="op">)</span> </span></code></pre></div>
<pre><code><span><span class="co">##   BFGS     NM   CGFR   CGPR   CGBS G-BFGS G-CGFR G-CGPR G-CGBS </span></span>
<span><span class="co">##     14     14     14     14     14     14     14     14     14</span></span></code></pre>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#get back to original parametrization</span></span>
<span><span class="va">expopt</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"fitted shape1"</span>, <span class="st">"fitted shape2"</span><span class="op">)</span>, <span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">expopt</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"fitted shape1"</span>, <span class="st">"fitted shape2"</span><span class="op">)</span>, <span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p>Then we extract the values of the fitted parameters, the value of the
corresponding log-likelihood and the number of counts to the function
to minimize and its gradient (whether it is the theoretical gradient or the
numerically approximated one).</p>
</div>
<div class="section level3" number="2.4">
<h3 id="results-of-the-numerical-investigation">
<span class="header-section-number">2.4</span> Results of the numerical investigation<a class="anchor" aria-label="anchor" href="#results-of-the-numerical-investigation"></a>
</h3>
<p>Results are displayed in the following tables:
(1) the original parametrization without specifying the gradient (<code>-B</code> stands for bounded version),
(2) the original parametrization with the (true) gradient (<code>-B</code> stands for bounded version and <code>-G</code> for gradient),
(3) the log-transformed parametrization without specifying the gradient,
(4) the log-transformed parametrization with the (true) gradient (<code>-G</code> stands for gradient).</p>
<table class="table">
<caption>
<span id="tab:unnamed-chunk-8">Table 2.1: </span>Unconstrained optimization with approximated gradient</caption>
<colgroup>
<col width="21%">
<col width="10%">
<col width="10%">
<col width="10%">
<col width="10%">
<col width="10%">
<col width="12%">
<col width="10%">
</colgroup>
<thead><tr class="header">
<th align="left"></th>
<th align="right">BFGS</th>
<th align="right">NM</th>
<th align="right">CGFR</th>
<th align="right">CGPR</th>
<th align="right">CGBS</th>
<th align="right">L-BFGS-B</th>
<th align="right">NM-B</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">fitted shape1</td>
<td align="right">2.665</td>
<td align="right">2.664</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
</tr>
<tr class="even">
<td align="left">fitted shape2</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
</tr>
<tr class="odd">
<td align="left">fitted loglik</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
</tr>
<tr class="even">
<td align="left">func. eval. nb.</td>
<td align="right">23.000</td>
<td align="right">47.000</td>
<td align="right">211.000</td>
<td align="right">263.000</td>
<td align="right">183.000</td>
<td align="right">11.000</td>
<td align="right">47.000</td>
</tr>
<tr class="odd">
<td align="left">grad. eval. nb.</td>
<td align="right">5.000</td>
<td align="right">NA</td>
<td align="right">53.000</td>
<td align="right">69.000</td>
<td align="right">47.000</td>
<td align="right">11.000</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">time (sec)</td>
<td align="right">0.005</td>
<td align="right">0.003</td>
<td align="right">0.023</td>
<td align="right">0.028</td>
<td align="right">0.019</td>
<td align="right">0.004</td>
<td align="right">0.007</td>
</tr>
</tbody>
</table>
<table class="table">
<caption>
<span id="tab:unnamed-chunk-9">Table 2.2: </span>Unconstrained optimization with true gradient</caption>
<colgroup>
<col width="17%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="8%">
<col width="9%">
<col width="8%">
<col width="9%">
<col width="9%">
<col width="9%">
</colgroup>
<thead><tr class="header">
<th align="left"></th>
<th align="right">G-BFGS</th>
<th align="right">G-CGFR</th>
<th align="right">G-CGPR</th>
<th align="right">G-CGBS</th>
<th align="right">G-BFGS-B</th>
<th align="right">G-NM-B</th>
<th align="right">G-CGFR-B</th>
<th align="right">G-CGPR-B</th>
<th align="right">G-CGBS-B</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">fitted shape1</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
</tr>
<tr class="even">
<td align="left">fitted shape2</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
</tr>
<tr class="odd">
<td align="left">fitted loglik</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
</tr>
<tr class="even">
<td align="left">func. eval. nb.</td>
<td align="right">20.000</td>
<td align="right">249.000</td>
<td align="right">225.000</td>
<td align="right">138.000</td>
<td align="right">25.000</td>
<td align="right">47.000</td>
<td align="right">263.000</td>
<td align="right">188.000</td>
<td align="right">176.000</td>
</tr>
<tr class="odd">
<td align="left">grad. eval. nb.</td>
<td align="right">5.000</td>
<td align="right">71.000</td>
<td align="right">69.000</td>
<td align="right">43.000</td>
<td align="right">5.000</td>
<td align="right">NA</td>
<td align="right">69.000</td>
<td align="right">59.000</td>
<td align="right">47.000</td>
</tr>
<tr class="even">
<td align="left">time (sec)</td>
<td align="right">0.009</td>
<td align="right">0.075</td>
<td align="right">0.074</td>
<td align="right">0.045</td>
<td align="right">0.014</td>
<td align="right">0.013</td>
<td align="right">0.086</td>
<td align="right">0.076</td>
<td align="right">0.065</td>
</tr>
</tbody>
</table>
<table class="table">
<caption>
<span id="tab:unnamed-chunk-10">Table 2.3: </span>Exponential trick optimization with approximated gradient</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="right">BFGS</th>
<th align="right">NM</th>
<th align="right">CGFR</th>
<th align="right">CGPR</th>
<th align="right">CGBS</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">fitted shape1</td>
<td align="right">2.665</td>
<td align="right">2.664</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
</tr>
<tr class="even">
<td align="left">fitted shape2</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
</tr>
<tr class="odd">
<td align="left">fitted loglik</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
</tr>
<tr class="even">
<td align="left">func. eval. nb.</td>
<td align="right">18.000</td>
<td align="right">41.000</td>
<td align="right">131.000</td>
<td align="right">116.000</td>
<td align="right">134.000</td>
</tr>
<tr class="odd">
<td align="left">grad. eval. nb.</td>
<td align="right">5.000</td>
<td align="right">NA</td>
<td align="right">27.000</td>
<td align="right">29.000</td>
<td align="right">35.000</td>
</tr>
<tr class="even">
<td align="left">time (sec)</td>
<td align="right">0.015</td>
<td align="right">0.004</td>
<td align="right">0.014</td>
<td align="right">0.013</td>
<td align="right">0.015</td>
</tr>
</tbody>
</table>
<table class="table">
<caption>
<span id="tab:unnamed-chunk-11">Table 2.4: </span>Exponential trick optimization with true gradient</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="right">G-BFGS</th>
<th align="right">G-CGFR</th>
<th align="right">G-CGPR</th>
<th align="right">G-CGBS</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">fitted shape1</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
<td align="right">2.665</td>
</tr>
<tr class="even">
<td align="left">fitted shape2</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
<td align="right">0.731</td>
</tr>
<tr class="odd">
<td align="left">fitted loglik</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
<td align="right">114.165</td>
</tr>
<tr class="even">
<td align="left">func. eval. nb.</td>
<td align="right">20.000</td>
<td align="right">175.000</td>
<td align="right">125.000</td>
<td align="right">112.000</td>
</tr>
<tr class="odd">
<td align="left">grad. eval. nb.</td>
<td align="right">5.000</td>
<td align="right">39.000</td>
<td align="right">41.000</td>
<td align="right">35.000</td>
</tr>
<tr class="even">
<td align="left">time (sec)</td>
<td align="right">0.011</td>
<td align="right">0.045</td>
<td align="right">0.043</td>
<td align="right">0.038</td>
</tr>
</tbody>
</table>
<p>Using <code>llsurface</code>, we plot the log-likehood surface around the true value (green) and the fitted parameters (red).</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/logLik-surface.html">llsurface</a></span><span class="op">(</span>min.arg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0.1</span>, <span class="fl">0.1</span><span class="op">)</span>, max.arg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">7</span>, <span class="fl">3</span><span class="op">)</span>, xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">.1</span>,<span class="fl">7</span><span class="op">)</span>, </span>
<span>          plot.arg<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"shape1"</span>, <span class="st">"shape2"</span><span class="op">)</span>, nlev<span class="op">=</span><span class="fl">25</span>,</span>
<span>          lseq<span class="op">=</span><span class="fl">50</span>, data<span class="op">=</span><span class="va">x</span>, distr<span class="op">=</span><span class="st">"beta"</span>, back.col <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html" class="external-link">points</a></span><span class="op">(</span><span class="va">unconstropt</span><span class="op">[</span><span class="fl">1</span>,<span class="st">"BFGS"</span><span class="op">]</span>, <span class="va">unconstropt</span><span class="op">[</span><span class="fl">2</span>,<span class="st">"BFGS"</span><span class="op">]</span>, pch<span class="op">=</span><span class="st">"+"</span>, col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html" class="external-link">points</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">/</span><span class="fl">4</span>, pch<span class="op">=</span><span class="st">"x"</span>, col<span class="op">=</span><span class="st">"green"</span><span class="op">)</span></span></code></pre></div>
<p><img src="Optimalgo_files/figure-html/unnamed-chunk-12-1.png"><!-- --></p>
<p>We can simulate bootstrap replicates using the <code>bootdist</code> function.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">b1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bootdist.html">bootdist</a></span><span class="op">(</span><span class="fu"><a href="../reference/fitdist.html">fitdist</a></span><span class="op">(</span><span class="va">x</span>, <span class="st">"beta"</span>, method <span class="op">=</span> <span class="st">"mle"</span>, optim.method <span class="op">=</span> <span class="st">"BFGS"</span><span class="op">)</span>, </span>
<span>               niter <span class="op">=</span> <span class="fl">100</span>, parallel <span class="op">=</span> <span class="st">"snow"</span>, ncpus <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">b1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Parametric bootstrap medians and 95% percentile CI </span></span>
<span><span class="co">##        Median  2.5% 97.5%</span></span>
<span><span class="co">## shape1   2.73 2.272 3.283</span></span>
<span><span class="co">## shape2   0.75 0.652 0.888</span></span></code></pre>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">b1</span>, trueval <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">3</span>, <span class="fl">3</span><span class="op">/</span><span class="fl">4</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="Optimalgo_files/figure-html/unnamed-chunk-13-1.png"><!-- --></p>
</div>
</div>
<div class="section level2" number="3">
<h2 id="numerical-illustration-with-the-negative-binomial-distribution">
<span class="header-section-number">3</span> Numerical illustration with the negative binomial distribution<a class="anchor" aria-label="anchor" href="#numerical-illustration-with-the-negative-binomial-distribution"></a>
</h2>
<div class="section level3" number="3.1">
<h3 id="log-likelihood-function-and-its-gradient-for-negative-binomial-distribution">
<span class="header-section-number">3.1</span> Log-likelihood function and its gradient for negative binomial distribution<a class="anchor" aria-label="anchor" href="#log-likelihood-function-and-its-gradient-for-negative-binomial-distribution"></a>
</h3>
<div class="section level4" number="3.1.1">
<h4 id="theoretical-value-1">
<span class="header-section-number">3.1.1</span> Theoretical value<a class="anchor" aria-label="anchor" href="#theoretical-value-1"></a>
</h4>
<p>The p.m.f. of the Negative binomial distribution is given by
<span class="math display">\[
f(x; m,p) = \frac{\Gamma(x+m)}{\Gamma(m)x!} p^m (1-p)^x,
\]</span>
where <span class="math inline">\(\Gamma\)</span> denotes the beta function, see the NIST Handbook of mathematical functions <a href="https://dlmf.nist.gov/" class="external-link uri">https://dlmf.nist.gov/</a>.
There exists an alternative representation where <span class="math inline">\(\mu=m (1-p)/p\)</span> or equivalently <span class="math inline">\(p=m/(m+\mu)\)</span>.
Thus, the log-likelihood for a set of observations <span class="math inline">\((x_1,\dots,x_n)\)</span> is
<span class="math display">\[
\log L(m,p) =
\sum_{i=1}^{n} \log\Gamma(x_i+m)
-n\log\Gamma(m)
-\sum_{i=1}^{n} \log(x_i!)
+ mn\log(p)
+\sum_{i=1}^{n} {x_i}\log(1-p)
\]</span>
The gradient with respect to <span class="math inline">\(m\)</span> and <span class="math inline">\(p\)</span> is
<span class="math display">\[
\nabla \log L(m,p) =
\left(\begin{matrix}
\sum_{i=1}^{n} \psi(x_i+m)
-n \psi(m)
+ n\log(p)
\\
mn/p
-\sum_{i=1}^{n} {x_i}/(1-p)
\end{matrix}\right),
\]</span>
where <span class="math inline">\(\psi(x)=\Gamma'(x)/\Gamma(x)\)</span> is the digamma function,
see the NIST Handbook of mathematical functions <a href="https://dlmf.nist.gov/" class="external-link uri">https://dlmf.nist.gov/</a>.</p>
</div>
<div class="section level4" number="3.1.2">
<h4 id="r-implementation-1">
<span class="header-section-number">3.1.2</span> <code>R</code> implementation<a class="anchor" aria-label="anchor" href="#r-implementation-1"></a>
</h4>
<p>As in the <code>fitdistrplus</code> package, we minimize the opposite of the log-likelihood: we implement the opposite of the gradient in <code>grlnL</code>.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">grlnlNB</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">obs</span>, <span class="va">...</span><span class="op">)</span></span>
<span><span class="op">{</span></span>
<span>  <span class="va">m</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span></span>
<span>  <span class="va">p</span> <span class="op">&lt;-</span> <span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span></span>
<span>  <span class="va">n</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">obs</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Special.html" class="external-link">psigamma</a></span><span class="op">(</span><span class="va">obs</span><span class="op">+</span><span class="va">m</span><span class="op">)</span><span class="op">)</span> <span class="op">-</span> <span class="va">n</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Special.html" class="external-link">psigamma</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span> <span class="op">+</span> <span class="va">n</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">p</span><span class="op">)</span>,</span>
<span>    <span class="va">m</span><span class="op">*</span><span class="va">n</span><span class="op">/</span><span class="va">p</span> <span class="op">-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html" class="external-link">sum</a></span><span class="op">(</span><span class="va">obs</span><span class="op">)</span><span class="op">/</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="va">p</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
</div>
</div>
<div class="section level3" number="3.2">
<h3 id="random-generation-of-a-sample-1">
<span class="header-section-number">3.2</span> Random generation of a sample<a class="anchor" aria-label="anchor" href="#random-generation-of-a-sample-1"></a>
</h3>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#(2) negative binomial distribution</span></span>
<span><span class="va">n</span> <span class="op">&lt;-</span> <span class="fl">200</span></span>
<span><span class="va">trueval</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"size"</span><span class="op">=</span><span class="fl">10</span>, <span class="st">"prob"</span><span class="op">=</span><span class="fl">3</span><span class="op">/</span><span class="fl">4</span>, <span class="st">"mu"</span><span class="op">=</span><span class="fl">10</span><span class="op">/</span><span class="fl">3</span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/NegBinomial.html" class="external-link">rnbinom</a></span><span class="op">(</span><span class="va">n</span>, <span class="va">trueval</span><span class="op">[</span><span class="st">"size"</span><span class="op">]</span>, <span class="va">trueval</span><span class="op">[</span><span class="st">"prob"</span><span class="op">]</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/hist.html" class="external-link">hist</a></span><span class="op">(</span><span class="va">x</span>, prob<span class="op">=</span><span class="cn">TRUE</span>, ylim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">.3</span><span class="op">)</span>, xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">10</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/density.html" class="external-link">density</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, col<span class="op">=</span><span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html" class="external-link">points</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/NegBinomial.html" class="external-link">dnbinom</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">x</span><span class="op">)</span>, <span class="va">trueval</span><span class="op">[</span><span class="st">"size"</span><span class="op">]</span>, <span class="va">trueval</span><span class="op">[</span><span class="st">"prob"</span><span class="op">]</span><span class="op">)</span>, </span>
<span>       col <span class="op">=</span> <span class="st">"green"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, lty <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"red"</span>, <span class="st">"green"</span><span class="op">)</span>, </span>
<span>       legend <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"empirical"</span>, <span class="st">"theoretical"</span><span class="op">)</span>, bty<span class="op">=</span><span class="st">"n"</span><span class="op">)</span></span></code></pre></div>
<p><img src="Optimalgo_files/figure-html/unnamed-chunk-15-1.png"><!-- --></p>
</div>
<div class="section level3" number="3.3">
<h3 id="fit-a-negative-binomial-distribution">
<span class="header-section-number">3.3</span> Fit a negative binomial distribution<a class="anchor" aria-label="anchor" href="#fit-a-negative-binomial-distribution"></a>
</h3>
<p>Define control parameters and make the benchmark.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">ctr</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>trace <span class="op">=</span> <span class="fl">0</span>, REPORT <span class="op">=</span> <span class="fl">1</span>, maxit <span class="op">=</span> <span class="fl">1000</span><span class="op">)</span></span>
<span><span class="va">unconstropt</span> <span class="op">&lt;-</span> <span class="fu">fitbench</span><span class="op">(</span><span class="va">x</span>, <span class="st">"nbinom"</span>, <span class="st">"mle"</span>, grad <span class="op">=</span> <span class="va">grlnlNB</span>, lower <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##     BFGS       NM     CGFR     CGPR     CGBS L-BFGS-B     NM-B   G-BFGS </span></span>
<span><span class="co">##       14       14       14       14       14       14       14       14 </span></span>
<span><span class="co">##   G-CGFR   G-CGPR   G-CGBS G-BFGS-B   G-NM-B G-CGFR-B G-CGPR-B G-CGBS-B </span></span>
<span><span class="co">##       14       14       14       14       14       14       14       14</span></span></code></pre>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">unconstropt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">unconstropt</span>, </span>
<span>                     <span class="st">"fitted prob"</span> <span class="op">=</span> <span class="va">unconstropt</span><span class="op">[</span><span class="st">"fitted mu"</span>, <span class="op">]</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">+</span> <span class="va">unconstropt</span><span class="op">[</span><span class="st">"fitted mu"</span>, <span class="op">]</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>In the case of constrained optimization, <code>mledist</code> permits the direct use
of <code>constrOptim</code> function (still implemented in <code>stats</code> package)
that allow linear inequality constraints by using a logarithmic barrier.</p>
<p>Use a exp/log transformation of the shape parameters <span class="math inline">\(\delta_1\)</span> and <span class="math inline">\(\delta_2\)</span>
to ensure that the shape parameters are strictly positive.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dnbinom2</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">size</span>, <span class="va">prob</span>, <span class="va">log</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/stats/NegBinomial.html" class="external-link">dnbinom</a></span><span class="op">(</span><span class="va">x</span>, <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">size</span><span class="op">)</span>, <span class="fl">1</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="op">-</span><span class="va">prob</span><span class="op">)</span><span class="op">)</span>, log <span class="op">=</span> <span class="va">log</span><span class="op">)</span></span>
<span><span class="co"># transform starting values</span></span>
<span><span class="va">startarg</span> <span class="op">&lt;-</span> <span class="fu">fitdistrplus</span><span class="fu">:::</span><span class="fu">startargdefault</span><span class="op">(</span><span class="va">x</span>, <span class="st">"nbinom"</span><span class="op">)</span></span>
<span><span class="va">startarg</span><span class="op">$</span><span class="va">mu</span> <span class="op">&lt;-</span> <span class="va">startarg</span><span class="op">$</span><span class="va">size</span> <span class="op">/</span> <span class="op">(</span><span class="va">startarg</span><span class="op">$</span><span class="va">size</span> <span class="op">+</span> <span class="va">startarg</span><span class="op">$</span><span class="va">mu</span><span class="op">)</span></span>
<span><span class="va">startarg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>size <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">startarg</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span>, </span>
<span>                 prob <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">log</a></span><span class="op">(</span><span class="va">startarg</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span> <span class="op">/</span> <span class="op">(</span><span class="fl">1</span> <span class="op">-</span> <span class="va">startarg</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># redefine the gradient for the new parametrization</span></span>
<span><span class="va">Trans</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html" class="external-link">plogis</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">grNBexp</span> <span class="op">&lt;-</span> <span class="kw">function</span><span class="op">(</span><span class="va">par</span>, <span class="va">obs</span>, <span class="va">...</span><span class="op">)</span> </span>
<span>    <span class="fu">grlnlNB</span><span class="op">(</span><span class="fu">Trans</span><span class="op">(</span><span class="va">par</span><span class="op">)</span>, <span class="va">obs</span><span class="op">)</span> <span class="op">*</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Log.html" class="external-link">exp</a></span><span class="op">(</span><span class="va">par</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html" class="external-link">plogis</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">*</span><span class="op">(</span><span class="fl">1</span><span class="op">-</span><span class="fu"><a href="https://rdrr.io/r/stats/Logistic.html" class="external-link">plogis</a></span><span class="op">(</span><span class="va">x</span><span class="op">[</span><span class="fl">2</span><span class="op">]</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="va">expopt</span> <span class="op">&lt;-</span> <span class="fu">fitbench</span><span class="op">(</span><span class="va">x</span>, distr<span class="op">=</span><span class="st">"nbinom2"</span>, method<span class="op">=</span><span class="st">"mle"</span>, grad<span class="op">=</span><span class="va">grNBexp</span>, start<span class="op">=</span><span class="va">startarg</span><span class="op">)</span> </span></code></pre></div>
<pre><code><span><span class="co">##   BFGS     NM   CGFR   CGPR   CGBS G-BFGS G-CGFR G-CGPR G-CGBS </span></span>
<span><span class="co">##     14     14     14     14     14     14     14     14     14</span></span></code></pre>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># get back to original parametrization</span></span>
<span><span class="va">expopt</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"fitted size"</span>, <span class="st">"fitted prob"</span><span class="op">)</span>, <span class="op">]</span> <span class="op">&lt;-</span> </span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/apply.html" class="external-link">apply</a></span><span class="op">(</span><span class="va">expopt</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"fitted size"</span>, <span class="st">"fitted prob"</span><span class="op">)</span>, <span class="op">]</span>, <span class="fl">2</span>, <span class="va">Trans</span><span class="op">)</span></span></code></pre></div>
<p>Then we extract the values of the fitted parameters, the value of the
corresponding log-likelihood and the number of counts to the function
to minimize and its gradient (whether it is the theoretical gradient or the
numerically approximated one).</p>
</div>
<div class="section level3" number="3.4">
<h3 id="results-of-the-numerical-investigation-1">
<span class="header-section-number">3.4</span> Results of the numerical investigation<a class="anchor" aria-label="anchor" href="#results-of-the-numerical-investigation-1"></a>
</h3>
<p>Results are displayed in the following tables:
(1) the original parametrization without specifying the gradient (<code>-B</code> stands for bounded version),
(2) the original parametrization with the (true) gradient (<code>-B</code> stands for bounded version and <code>-G</code> for gradient),
(3) the log-transformed parametrization without specifying the gradient,
(4) the log-transformed parametrization with the (true) gradient (<code>-G</code> stands for gradient).</p>
<table style="width:100%;" class="table">
<caption>
<span id="tab:unnamed-chunk-18">Table 3.1: </span>Unconstrained optimization with approximated gradient</caption>
<colgroup>
<col width="20%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
<col width="11%">
</colgroup>
<thead><tr class="header">
<th align="left"></th>
<th align="right">BFGS</th>
<th align="right">NM</th>
<th align="right">CGFR</th>
<th align="right">CGPR</th>
<th align="right">CGBS</th>
<th align="right">L-BFGS-B</th>
<th align="right">NM-B</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">fitted size</td>
<td align="right">61.944</td>
<td align="right">68.138</td>
<td align="right">61.969</td>
<td align="right">62.364</td>
<td align="right">62.878</td>
<td align="right">61.944</td>
<td align="right">67.397</td>
</tr>
<tr class="even">
<td align="left">fitted mu</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
</tr>
<tr class="odd">
<td align="left">fitted loglik</td>
<td align="right">-401.612</td>
<td align="right">-401.611</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.611</td>
</tr>
<tr class="even">
<td align="left">func. eval. nb.</td>
<td align="right">2.000</td>
<td align="right">37.000</td>
<td align="right">2999.000</td>
<td align="right">2680.000</td>
<td align="right">2610.000</td>
<td align="right">2.000</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="left">grad. eval. nb.</td>
<td align="right">1.000</td>
<td align="right">NA</td>
<td align="right">1001.000</td>
<td align="right">1001.000</td>
<td align="right">1001.000</td>
<td align="right">2.000</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">time (sec)</td>
<td align="right">0.001</td>
<td align="right">0.002</td>
<td align="right">0.221</td>
<td align="right">0.206</td>
<td align="right">0.203</td>
<td align="right">0.001</td>
<td align="right">0.003</td>
</tr>
<tr class="odd">
<td align="left">fitted prob</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
</tr>
</tbody>
</table>
<table class="table">
<caption>
<span id="tab:unnamed-chunk-19">Table 3.2: </span>Unconstrained optimization with true gradient</caption>
<colgroup>
<col width="16%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
<col width="9%">
</colgroup>
<thead><tr class="header">
<th align="left"></th>
<th align="right">G-BFGS</th>
<th align="right">G-CGFR</th>
<th align="right">G-CGPR</th>
<th align="right">G-CGBS</th>
<th align="right">G-BFGS-B</th>
<th align="right">G-NM-B</th>
<th align="right">G-CGFR-B</th>
<th align="right">G-CGPR-B</th>
<th align="right">G-CGBS-B</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">fitted size</td>
<td align="right">61.944</td>
<td align="right">61.944</td>
<td align="right">61.944</td>
<td align="right">61.944</td>
<td align="right">61.944</td>
<td align="right">67.397</td>
<td align="right">61.944</td>
<td align="right">61.944</td>
<td align="right">61.944</td>
</tr>
<tr class="even">
<td align="left">fitted mu</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
<td align="right">3.425</td>
</tr>
<tr class="odd">
<td align="left">fitted loglik</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.611</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
</tr>
<tr class="even">
<td align="left">func. eval. nb.</td>
<td align="right">26.000</td>
<td align="right">881.000</td>
<td align="right">174.000</td>
<td align="right">881.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
<td align="right">0.000</td>
</tr>
<tr class="odd">
<td align="left">grad. eval. nb.</td>
<td align="right">1.000</td>
<td align="right">59.000</td>
<td align="right">11.000</td>
<td align="right">59.000</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">NA</td>
</tr>
<tr class="even">
<td align="left">time (sec)</td>
<td align="right">0.009</td>
<td align="right">0.032</td>
<td align="right">0.007</td>
<td align="right">0.031</td>
<td align="right">0.002</td>
<td align="right">0.002</td>
<td align="right">0.031</td>
<td align="right">0.007</td>
<td align="right">0.031</td>
</tr>
<tr class="odd">
<td align="left">fitted prob</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
<td align="right">0.774</td>
</tr>
</tbody>
</table>
<table class="table">
<caption>
<span id="tab:unnamed-chunk-20">Table 3.3: </span>Exponential trick optimization with approximated gradient</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="right">BFGS</th>
<th align="right">NM</th>
<th align="right">CGFR</th>
<th align="right">CGPR</th>
<th align="right">CGBS</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">fitted size</td>
<td align="right">61.946</td>
<td align="right">67.787</td>
<td align="right">63.450</td>
<td align="right">67.941</td>
<td align="right">67.884</td>
</tr>
<tr class="even">
<td align="left">fitted prob</td>
<td align="right">0.948</td>
<td align="right">0.952</td>
<td align="right">0.949</td>
<td align="right">0.952</td>
<td align="right">0.952</td>
</tr>
<tr class="odd">
<td align="left">fitted loglik</td>
<td align="right">-401.612</td>
<td align="right">-401.611</td>
<td align="right">-401.612</td>
<td align="right">-401.611</td>
<td align="right">-401.611</td>
</tr>
<tr class="even">
<td align="left">func. eval. nb.</td>
<td align="right">6.000</td>
<td align="right">47.000</td>
<td align="right">4001.000</td>
<td align="right">3721.000</td>
<td align="right">381.000</td>
</tr>
<tr class="odd">
<td align="left">grad. eval. nb.</td>
<td align="right">1.000</td>
<td align="right">NA</td>
<td align="right">1001.000</td>
<td align="right">1001.000</td>
<td align="right">97.000</td>
</tr>
<tr class="even">
<td align="left">time (sec)</td>
<td align="right">0.004</td>
<td align="right">0.002</td>
<td align="right">0.258</td>
<td align="right">0.234</td>
<td align="right">0.025</td>
</tr>
</tbody>
</table>
<table class="table">
<caption>
<span id="tab:unnamed-chunk-21">Table 3.4: </span>Exponential trick optimization with true gradient</caption>
<thead><tr class="header">
<th align="left"></th>
<th align="right">G-BFGS</th>
<th align="right">G-CGFR</th>
<th align="right">G-CGPR</th>
<th align="right">G-CGBS</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">fitted size</td>
<td align="right">61.944</td>
<td align="right">61.944</td>
<td align="right">61.944</td>
<td align="right">61.944</td>
</tr>
<tr class="even">
<td align="left">fitted prob</td>
<td align="right">0.948</td>
<td align="right">0.948</td>
<td align="right">0.948</td>
<td align="right">0.948</td>
</tr>
<tr class="odd">
<td align="left">fitted loglik</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
<td align="right">-401.612</td>
</tr>
<tr class="even">
<td align="left">func. eval. nb.</td>
<td align="right">21.000</td>
<td align="right">43.000</td>
<td align="right">42.000</td>
<td align="right">42.000</td>
</tr>
<tr class="odd">
<td align="left">grad. eval. nb.</td>
<td align="right">1.000</td>
<td align="right">3.000</td>
<td align="right">3.000</td>
<td align="right">3.000</td>
</tr>
<tr class="even">
<td align="left">time (sec)</td>
<td align="right">0.006</td>
<td align="right">0.002</td>
<td align="right">0.002</td>
<td align="right">0.002</td>
</tr>
</tbody>
</table>
<p>Using <code>llsurface</code>, we plot the log-likehood surface around the true value (green) and the fitted parameters (red).</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/logLik-surface.html">llsurface</a></span><span class="op">(</span>min.arg <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">0.3</span><span class="op">)</span>, max.arg <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">15</span>, <span class="fl">1</span><span class="op">)</span>, xlim<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">5</span>, <span class="fl">15</span><span class="op">)</span>,</span>
<span>          plot.arg <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"size"</span>, <span class="st">"prob"</span><span class="op">)</span>, nlev <span class="op">=</span> <span class="fl">25</span>,</span>
<span>          lseq <span class="op">=</span> <span class="fl">50</span>, data <span class="op">=</span> <span class="va">x</span>, distr <span class="op">=</span> <span class="st">"nbinom"</span>, back.col <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html" class="external-link">points</a></span><span class="op">(</span><span class="va">unconstropt</span><span class="op">[</span><span class="st">"fitted size"</span>, <span class="st">"BFGS"</span><span class="op">]</span>, <span class="va">unconstropt</span><span class="op">[</span><span class="st">"fitted prob"</span>, <span class="st">"BFGS"</span><span class="op">]</span>, </span>
<span>       pch <span class="op">=</span> <span class="st">"+"</span>, col <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/points.html" class="external-link">points</a></span><span class="op">(</span><span class="va">trueval</span><span class="op">[</span><span class="st">"size"</span><span class="op">]</span>, <span class="va">trueval</span><span class="op">[</span><span class="st">"prob"</span><span class="op">]</span>, pch <span class="op">=</span> <span class="st">"x"</span>, col <span class="op">=</span> <span class="st">"green"</span><span class="op">)</span></span></code></pre></div>
<p><img src="Optimalgo_files/figure-html/unnamed-chunk-22-1.png"><!-- --></p>
<p>We can simulate bootstrap replicates using the <code>bootdist</code> function.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">b1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bootdist.html">bootdist</a></span><span class="op">(</span><span class="fu"><a href="../reference/fitdist.html">fitdist</a></span><span class="op">(</span><span class="va">x</span>, <span class="st">"nbinom"</span>, method <span class="op">=</span> <span class="st">"mle"</span>, optim.method <span class="op">=</span> <span class="st">"BFGS"</span><span class="op">)</span>, </span>
<span>               niter <span class="op">=</span> <span class="fl">100</span>, parallel <span class="op">=</span> <span class="st">"snow"</span>, ncpus <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">b1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Parametric bootstrap medians and 95% percentile CI </span></span>
<span><span class="co">##      Median  2.5%  97.5%</span></span>
<span><span class="co">## size  61.95 11.05 118.32</span></span>
<span><span class="co">## mu     3.43  3.17   3.72</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## The estimation method converged only for 76 among 100 iterations</span></span></code></pre>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">b1</span>, trueval<span class="op">=</span><span class="va">trueval</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"size"</span>, <span class="st">"mu"</span><span class="op">)</span><span class="op">]</span><span class="op">)</span> </span></code></pre></div>
<p><img src="Optimalgo_files/figure-html/unnamed-chunk-23-1.png"><!-- --></p>
</div>
</div>
<div class="section level2" number="4">
<h2 id="conclusion">
<span class="header-section-number">4</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"></a>
</h2>
<p>Based on the two previous examples, we observe that all methods converge to the same
point. This is reassuring.<br>
However, the number of function evaluations (and the gradient evaluations) is
very different from a method to another.
Furthermore, specifying the true gradient of the log-likelihood does not
help at all the fitting procedure and generally slows down the convergence.
Generally, the best method is the standard BFGS method or the BFGS method
with the exponential transformation of the parameters.
Since the exponential function is differentiable, the asymptotic properties are
still preserved (by the Delta method) but for finite-sample this may produce a small bias.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by <a href="https://lbbe.univ-lyon1.fr/fr/annuaires-des-membres/delignette-muller-marie-laure" class="external-link">Marie-Laure Delignette-Muller</a>, <a href="http://dutangc.free.fr" class="external-link">Christophe Dutang</a>, <a href="https://lbbe.univ-lyon1.fr/fr/annuaires-des-membres/siberchicot-aurelie" class="external-link">Aurélie Siberchicot</a>.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
